{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c295a2-0371-46ce-9ad2-c2b53156c20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b8d3b29b70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(2697134)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23577a3-0151-4d08-8f16-25e8045333e1",
   "metadata": {},
   "source": [
    "### Explanation of WatermarkDataset Class\n",
    "The `WatermarkDataset` class is a custom dataset implementation designed for handling datasets of watermarked images and their corresponding target (clear) images. It leverages a CSV file to retrieve the paths of images stored in a specified root directory, ensuring flexibility in dataset organization. The class resizes images to a given size, converts them to RGB format for consistency, and supports applying optional transformations, such as normalization or augmentation, to both input and target images. It includes a subsampling mechanism that allows users to skip over samples in the dataset, which is particularly useful for working with large datasets and enabling faster iterations during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43dd254-9148-45f6-8a3e-a8218c1b4a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatermarkDataset(Dataset):\n",
    "    def __init__(self, csv_file_path, database_root_dir, image_size, subsample=1, transform=None):\n",
    "        self.csvFile = pd.read_csv(csv_file_path)\n",
    "        self.database_root_dir = database_root_dir\n",
    "        self.image_size = image_size\n",
    "        self.subsample = subsample\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csvFile) // self.subsample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self):\n",
    "            raise IndexError(\"Index out of bounds\")\n",
    "        actual_idx = idx * self.subsample\n",
    "\n",
    "        input_image_path = os.path.join(self.database_root_dir, self.csvFile.iloc[actual_idx, 1])\n",
    "        target_image_path = os.path.join(self.database_root_dir, self.csvFile.iloc[actual_idx, 0])\n",
    "\n",
    "        input_image = Image.open(input_image_path).convert('RGB').resize((self.image_size, self.image_size))\n",
    "        target_image = Image.open(target_image_path).convert('RGB').resize((self.image_size, self.image_size))\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            target_image = self.transform(target_image)\n",
    "\n",
    "        return input_image, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098b6af-53f8-4a0a-b579-6ead14051b56",
   "metadata": {},
   "source": [
    "### Explanation of DoubleConv, Down, Up, OutConv, and VAE Classes\r\n",
    "\r\n",
    "The `DoubleConv`, `Down`, `Up`, and `OutConv` classes serve as fundamental components in constructing the encoder-decoder architecture utilized in the `VAE` class. These modules facilitate various stages of feature extraction, downsampling, upsampling, and output generation, forming the backbone of the Variational Autoencoder (VAE) designed specifically for watermark removal.\r\n",
    "\r\n",
    "- **DoubleConv** acts as a core building block that sequentially applies two convolutional layers, each followed by Batch Normalization and ReLU activation. This ensures effective feature extraction with normalization for stable training.\r\n",
    "- **Down** performs downsampling using a MaxPooling layer, followed by a `DoubleConv` module for extracting hierarchical features while reducing spatial dimensions.\r\n",
    "- **Up** focuses on upsampling the feature maps using a transposed convolution, followed by concatenation with skip connections from the encoder. It then applies a `DoubleConv` to refine the upsampled features, enabling high-resolution reconstruction.\r\n",
    "- **OutConv** is responsible for producing the final output of the model by reducing the number of channels and applying a Sigmoid activation. This is particularly suited for pixel-level predictions, such as mask generation or image reconstruction.\r\n",
    "- **VAE** integrates these components within an encoder-decoder framework and incorporates a latent space for compact representation. Additionally, it leverages a pre-trained YOLO model to identify watermark regions, using these predictions as an extra input channel to enhance the reconstruction process in masked areas.\r\n",
    "\r\n",
    "The VAE model operates through three primary stages: detecting watermarked regions via YOLO, encoding the input image into a latent representation, and decoding this representation to reconstruct a watermark-free image. This modular design, combining feature extraction, downsampling, and upsampling with guidance from YOLO, enables precise and efficient watermar removal.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98b834a-00fb-4b00-9e0b-97079678e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Define the DoubleConv module: Two convolutional layers with BatchNorm and ReLU\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A sequential module consisting of two convolutional layers, \n",
    "    each followed by BatchNorm and ReLU activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the DoubleConv module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DoubleConv module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after two convolutions.\n",
    "        \"\"\"\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the Down module: MaxPooling followed by DoubleConv\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    A module for downsampling that applies MaxPooling followed by a DoubleConv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the Down module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),  # Downsample by a factor of 2\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Down module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Downsampled tensor after MaxPooling and DoubleConv.\n",
    "        \"\"\"\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the Up module: Upsampling followed by DoubleConv\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    A module for upsampling that applies ConvTranspose2d for resizing, \n",
    "    concatenates with skip connections, and applies DoubleConv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the Up module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Forward pass of the Up module.\n",
    "\n",
    "        Args:\n",
    "            x1 (torch.Tensor): Tensor from the decoder path (upsampled).\n",
    "            x2 (torch.Tensor): Tensor from the encoder path (skip connection).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after upsampling and concatenation.\n",
    "        \"\"\"\n",
    "        # Perform upsampling\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # Handle size mismatch due to rounding in upsampling\n",
    "        diffY = x2.size(2) - x1.size(2)\n",
    "        diffX = x2.size(3) - x1.size(3)\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        # Concatenate and apply DoubleConv\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the OutConv module: Final output layer with Sigmoid activation\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A module for reducing the number of channels to the desired output \n",
    "    and applying Sigmoid activation for pixel-wise predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the OutConv module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),  # 1x1 convolution\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the OutConv module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with reduced channels.\n",
    "        \"\"\"\n",
    "        return self.conv(x)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the VAE (Variational Autoencoder) Model\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom Variational Autoencoder (VAE) model for watermark removal. \n",
    "    Integrates a YOLO object detection model for mask prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=4, out_channels=3, latent_dim=256, input_size=128, yolo_model_path=\"yolov8s.pt\", device='cuda'):\n",
    "        \"\"\"\n",
    "        Initialize the VAE model.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels (e.g., 4 for RGB + Mask).\n",
    "            out_channels (int): Number of output channels (e.g., 3 for RGB).\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "            input_size (int): Size of the input image (height and width).\n",
    "            yolo_model_path (str): Path to the pre-trained YOLO model.\n",
    "            device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        yolo = YOLO(yolo_model_path)\n",
    "        object.__setattr__(self, 'yolo_model', yolo)\n",
    "        self.device = device\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Encoder layers\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(1024 * (input_size // 16) ** 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(1024 * (input_size // 16) ** 2, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, 1024 * (input_size // 16) ** 2)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, out_channels)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick for the latent space.\n",
    "\n",
    "        Args:\n",
    "            mu (torch.Tensor): Mean of the latent distribution.\n",
    "            logvar (torch.Tensor): Log variance of the latent distribution.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Sampled latent vector.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def create_mask_from_yolo_preds(self, predictions, image_size):\n",
    "        \"\"\"\n",
    "        Create a binary mask from YOLO predictions.\n",
    "\n",
    "        Args:\n",
    "            predictions: YOLO detection results.\n",
    "            image_size (int): Size of the output mask (H, W).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Binary mask of shape (1, H, W).\n",
    "        \"\"\"\n",
    "        mask = torch.zeros((1, image_size, image_size))\n",
    "        for det in predictions:\n",
    "            boxes = det.boxes\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                mask[:, y1:y2, x1:x2] = 1.0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, input_images):\n",
    "        \"\"\"\n",
    "        Forward pass of the VAE model.\n",
    "\n",
    "        Args:\n",
    "            input_images (torch.Tensor): Batch of input images.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (Final output, mean, log variance, predicted masks).\n",
    "        \"\"\"\n",
    "        # YOLO mask prediction\n",
    "        batch_size = input_images.size(0)\n",
    "        predicted_masks = []\n",
    "        for i in range(batch_size):\n",
    "            img_np = (input_images[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            results = self.yolo_model.predict(img_np, imgsz=self.input_size, verbose=False, device=self.device)\n",
    "            pmask = self.create_mask_from_yolo_preds(results, self.input_size).to(input_images.device)\n",
    "            predicted_masks.append(pmask)\n",
    "        predicted_masks = torch.stack(predicted_masks, dim=0)\n",
    "\n",
    "        # Combine input with predicted masks\n",
    "        combined_input = torch.cat([input_images, predicted_masks], dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        x1 = self.inc(combined_input)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Latent space\n",
    "        x_flat = x5.view(batch_size, -1)\n",
    "        mu = torch.clamp(self.fc_mu(x_flat), -10, 10)\n",
    "        logvar = torch.clamp(self.fc_logvar(x_flat), -10, 10)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decoder\n",
    "        x_decoded = self.fc_dec(z).view(batch_size, 1024, x5.size(2), x5.size(3))\n",
    "        x = self.up1(x_decoded, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        reconstruction = self.outc(x)\n",
    "\n",
    "        # Apply mask blending\n",
    "        predicted_masks_3 = predicted_masks.expand_as(reconstruction)\n",
    "        final_output = input_images * (1 - predicted_masks_3) + reconstruction * predicted_masks_3\n",
    "\n",
    "        return final_output, mu, logvar, predicted_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c36672-958d-48a9-875f-2621eea2fb2d",
   "metadata": {},
   "source": [
    "### Explanation of VAELoss Class\r\n",
    "\r\n",
    "The `VAELoss` class is a specialized loss function designed for the Variational Autoencoder (VAE) model to tackle watermark removal tasks. It integrates two key components: reconstruction loss and KL divergence loss, each playing a distinct role in enhancing the model's performance. The reconstruction loss focuses on minimizing pixel-wise differences between the reconstructed images and the target (ground truth) images. To ensure the model pays more attention to watermarked regions, the loss assigns higher weights to these areas using a weight map derived from the difference between the input and target images. This approach allows the model to prioritize the challenging regions affected by watermarks during training.\r\n",
    "\r\n",
    "The second component, KL divergence loss, promotes regularization by encouraging the latent space to follow a standard normal distribution. This helps maintain a compact and stable latent representation, balancing reconstruction accuracy with effective regularization. The impact of KL divergence is controlled through a weighting factor (`kl_factor`), allowing flexibility in adjusting its influence during training. By combining these two components, `VAELoss` effectively guides the VAE model to achieve precise watermark removal while ensuring the latent space remains well-regularized and generalizable. This design makes it an essential component for training the VAE model to handle complex watermark removal challenes.\r\n",
    "egularized latent space.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac66efa-894c-47e6-bce0-580c1035a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss function for the VAE model.\n",
    "    \n",
    "    The loss combines:\n",
    "        1. Reconstruction Loss: Measures the difference between reconstructed images and target images,\n",
    "           with a higher focus on watermarked regions.\n",
    "        2. KL Divergence Loss: Encourages the latent distribution to match a standard normal distribution.\n",
    "\n",
    "    Watermarked regions are identified using the predicted mask, and additional weights are applied to \n",
    "    emphasize these areas during the reconstruction loss calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the VAELoss class.\n",
    "        \"\"\"\n",
    "        super(VAELoss, self).__init__()\n",
    "\n",
    "    def forward(self, reconstructed_images, target_images, input_images, mu, logvar, predicted_mask, kl_factor):\n",
    "        \"\"\"\n",
    "        Calculate the total loss for the VAE model.\n",
    "\n",
    "        Args:\n",
    "            reconstructed_images (torch.Tensor): Reconstructed output images of shape (B, C, H, W).\n",
    "            target_images (torch.Tensor): Ground truth target images of shape (B, C, H, W).\n",
    "            input_images (torch.Tensor): Original input images of shape (B, C, H, W).\n",
    "            mu (torch.Tensor): Mean of the latent space distribution of shape (B, latent_dim).\n",
    "            logvar (torch.Tensor): Log variance of the latent space distribution of shape (B, latent_dim).\n",
    "            predicted_mask (torch.Tensor): Predicted binary masks of shape (B, 1, H, W).\n",
    "            kl_factor (float): Weighting factor for the KL divergence loss.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Total loss value (scalar).\n",
    "        \"\"\"\n",
    "        eps = 1e-8  # A small epsilon value to prevent division by zero\n",
    "\n",
    "        # Step 1: Compute the mask for watermarked regions\n",
    "        # Convert predicted masks to binary (thresholded at 0.5)\n",
    "        mask = (predicted_mask > 0.5).float()\n",
    "        masked_pixels = mask.sum() + eps  # Count the number of masked pixels\n",
    "\n",
    "        # Step 2: Compute the difference between input and target images\n",
    "        # This identifies regions likely containing watermarks\n",
    "        diff_input_target = (input_images - target_images).abs()\n",
    "\n",
    "        # Step 3: Apply additional weights to watermarked regions\n",
    "        # Regions with larger differences (greater than 0.1) are given higher weight\n",
    "        watermark_weight = (diff_input_target > 0.1).float() * 5 + 1  # Threshold of 0.1\n",
    "\n",
    "        # Step 4: Compute the reconstruction loss\n",
    "        # Calculate the pixel-wise squared difference between reconstructed and target images\n",
    "        diff_recon_target = (reconstructed_images - target_images) ** 2\n",
    "\n",
    "        # Apply the mask and watermark weights to the reconstruction loss\n",
    "        weighted_diff = diff_recon_target * mask * watermark_weight\n",
    "        recon_loss = weighted_diff.sum() / masked_pixels  # Normalize by the number of masked pixels\n",
    "\n",
    "        # Step 5: Compute the KL divergence loss\n",
    "        # Encourages the latent space distribution to approach a standard normal distribution\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / (reconstructed_images.size(0) + eps)\n",
    "\n",
    "        # Step 6: Combine the losses\n",
    "        # The reconstruction loss is weighted higher (by a factor of 5)\n",
    "        total_loss = recon_loss * 5 + kl_factor * KLD\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a870424-1814-4f3f-9822-1dc8d4d2c8d7",
   "metadata": {},
   "source": [
    "### Explanation of `validateModel` Function\r\n",
    "\r\n",
    "The `validateModel` function is responsible for evaluating the Variational Autoencoder (VAE) model's performance on a validation dataset by calculating the average loss over all batches. This function is integral to monitoring the model's generalization ability during training.\r\n",
    "\r\n",
    "The function begins by setting the model to evaluation mode using `model.eval()`. This ensures that layers such as dropout and batch normalization behave correctly, as they are only intended for training. To optimize performance and reduce memory consumption, the function employs the `torch.no_grad()` context, which disables gradient computation during validation.\r\n",
    "\r\n",
    "For each batch in the validation dataset, the function calculates the loss using the provided custom loss function, such as `VAELoss`. This loss typically combines reconstruction loss, which measures how well the model reconstructs input images, and KL divergence loss, which regularizes the latent space. The `kl_factor` argument determines the weight of the KL divergence term, allowing fine-tuned control over its impact.\r\n",
    "\r\n",
    "The function iterates through all batches in the validation DataLoader, accumulating the loss values for each batch. Once all batches have been processed, the average validation loss is computed and returned. This average loss serves as a key metric for assessing the model's performance on unseen data and guiding decisions during hyperparameter tuning and training.\r\n",
    "\r\n",
    "Overall, the `validateModel` function provides an efficient and reliable way to evaluate the VAE model, ensuring that it generalizes well to the validation dataset without requiring additional training or gradien updates.\r\n",
    "\n",
    "r tuning.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f6841d7-ae23-4711-9959-ae4f994b7caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateModel(model, validation_loader, loss_func, device, kl_factor):\n",
    "    \"\"\"\n",
    "    Validate the VAE model on the validation dataset.\n",
    "\n",
    "    This function evaluates the model's performance on the validation dataset\n",
    "    by calculating the average validation loss over all batches. The loss is\n",
    "    computed using the specified loss function, which includes reconstruction\n",
    "    and KL divergence losses.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The VAE model to be validated.\n",
    "        validation_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        loss_func (callable): Custom loss function (e.g., VAELoss) for computing the loss.\n",
    "        device (str): Device to run the validation on ('cuda' or 'cpu').\n",
    "        kl_factor (float): Weighting factor for the KL divergence loss.\n",
    "\n",
    "    Returns:\n",
    "        float: The average validation loss over all batches.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode (disables dropout, batch norm updates, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    # List to store the loss for each batch\n",
    "    val_losses = []\n",
    "\n",
    "    # Disable gradient computation for validation to save memory and computation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the validation dataset\n",
    "        for input_images, target_images in validation_loader:\n",
    "            # Move input and target images to the specified device (e.g., GPU or CPU)\n",
    "            input_images = input_images.to(device)\n",
    "            target_images = target_images.to(device)\n",
    "\n",
    "            # Perform a forward pass through the VAE model\n",
    "            recon_images, mu, logvar, predicted_masks = model(input_images)\n",
    "\n",
    "            # Compute the loss using the provided loss function\n",
    "            # Includes reconstruction loss and KL divergence loss\n",
    "            loss = loss_func(recon_images, target_images, input_images, mu, logvar, predicted_masks, kl_factor=kl_factor)\n",
    "\n",
    "            # Append the computed loss for this batch to the list\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    # Compute and return the average validation loss over all batches\n",
    "    return np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6980f6-ec08-47eb-a4c9-e3bfa10b49bb",
   "metadata": {},
   "source": [
    "### Explanation of `trainModel` Function\r\n",
    "\r\n",
    "The `trainModel` function is designed to train the Variational Autoencoder (VAE) model using K-Fold Cross Validation, a robust technique for evaluating model performance. By dividing the dataset into multiple folds, this method ensures that the model is trained and validated on different subsets, improving its ability to generalize.\r\n",
    "\r\n",
    "The function begins by splitting the dataset into `num_folds` parts. For each fold, one subset is used for validation while the remaining subsets are used for training. This iterative process continues until every fold has been used as a validation set, providing a comprehensive evaluation of the model's performance.\r\n",
    "\r\n",
    "The training process uses the `VAELoss` function, which combines reconstruction loss and KL divergence loss. A warm-up period for the KL divergence factor (`kl_factor`) allows the model to focus on reconstruction in the early epochs before gradually incorporating the regularization effect of KL divergence. This dynamic adjustment improves training stability and performance.\r\n",
    "\r\n",
    "To adapt the learning rate dynamically, the function employs a scheduler, `ReduceLROnPlateau`, which reduces the learning rate when validation loss plateaus. This ensures efficient optimization and helps avoid overfitting.\r\n",
    "\r\n",
    "Throughout the training process, the function logs metrics such as average training and validation losses for each epoch. These metrics are stored for visualization and analysis, providing insights into the model's learning behavior. Additionally, the function saves the trained model weights and loss plots in the `outputs` directory for further use.\r\n",
    "\r\n",
    "The `trainModel` function integrates all essential components for effective training and evaluation, ensuring that the VAE model achieves high performance and generalizes well to nseen data.\r\n",
    "cross epochs.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ee48250-39ec-4fe9-b43c-d9eeae8493cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, optim, dataset, n_epochs, mini_batch_size, num_folds, device, model_save_name):\n",
    "    \"\"\"\n",
    "    Train the VAE model using K-Fold Cross Validation.\n",
    "\n",
    "    This function trains the model over multiple folds, where the dataset is split\n",
    "    into training and validation subsets for each fold. The training loss and\n",
    "    validation loss are logged and visualized, and the best-performing model is saved.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The VAE model to be trained.\n",
    "        optim (torch.optim.Optimizer): Optimizer for model training.\n",
    "        dataset (Dataset): Dataset used for training and validation.\n",
    "        n_epochs (int): Number of epochs to train the model.\n",
    "        mini_batch_size (int): Number of samples per batch.\n",
    "        num_folds (int): Number of folds for K-Fold Cross Validation.\n",
    "        device (str): Device to train the model on ('cuda' or 'cpu').\n",
    "        model_save_name (str): Base name for saving the trained model and outputs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    print(f\"Training on device: {device}\")\n",
    "\n",
    "    # Define the custom loss function\n",
    "    loss_func = VAELoss()\n",
    "\n",
    "    # Set up the K-Fold Cross Validator\n",
    "    k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=2697134)\n",
    "\n",
    "    # Learning rate scheduler to reduce LR on plateau\n",
    "    scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    # Create an output folder to save model weights and visualizations\n",
    "    output_folder = \"outputs\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Lists to store training and validation losses\n",
    "    training_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Loop through each fold\n",
    "    for fold, (train_ids, val_ids) in enumerate(k_fold.split(dataset)):\n",
    "        print(f'\\n{model_save_name} Fold {fold + 1}/{num_folds}')\n",
    "\n",
    "        # Define the train and validation samplers for the current fold\n",
    "        train_subsampler = SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = SubsetRandomSampler(val_ids)\n",
    "\n",
    "        # Create DataLoaders for training and validation\n",
    "        training_loader = DataLoader(dataset, batch_size=mini_batch_size, sampler=train_subsampler, num_workers=0, pin_memory=True)\n",
    "        validation_loader = DataLoader(dataset, batch_size=mini_batch_size, sampler=val_subsampler, num_workers=0, pin_memory=True)\n",
    "\n",
    "        # Store losses for the current fold\n",
    "        fold_training_losses = []\n",
    "        fold_val_losses = []\n",
    "\n",
    "        # Training loop for each epoch\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()  # Set the model to training mode\n",
    "\n",
    "            # Calculate KL divergence warm-up factor\n",
    "            warmup_epochs = int(n_epochs * 0.75)\n",
    "            max_kl_factor = 0.5\n",
    "            kl_factor = epoch / float(warmup_epochs) if epoch < warmup_epochs else max_kl_factor\n",
    "\n",
    "            batch_losses = []  # List to store batch losses\n",
    "            for batch_idx, (input_images, target_images) in enumerate(training_loader):\n",
    "                # Move data to the specified device\n",
    "                input_images = input_images.to(device)\n",
    "                target_images = target_images.to(device)\n",
    "\n",
    "                # Zero the gradients\n",
    "                optim.zero_grad()\n",
    "\n",
    "                # Forward pass through the model\n",
    "                recon_images, mu, logvar, predicted_masks = model(input_images)\n",
    "\n",
    "                # Compute the loss\n",
    "                loss = loss_func(recon_images, target_images, input_images, mu, logvar, predicted_masks, kl_factor=kl_factor)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "\n",
    "                # Store the batch loss\n",
    "                batch_losses.append(loss.item())\n",
    "\n",
    "            # Calculate and log average training loss for the epoch\n",
    "            epoch_loss = np.mean(batch_losses)\n",
    "            fold_training_losses.append(epoch_loss)\n",
    "            print(f\"\\tEpoch {epoch + 1}/{n_epochs}\")\n",
    "            print(f\"\\t\\tTraining Loss: {epoch_loss:.6f}\")\n",
    "\n",
    "            # Perform validation and compute validation loss\n",
    "            val_loss = validateModel(model, validation_loader, loss_func, device, kl_factor=kl_factor)\n",
    "            fold_val_losses.append(val_loss)\n",
    "            print(f\"\\t\\tValidation Loss: {val_loss:.6f}\")\n",
    "\n",
    "            # Update the learning rate scheduler\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # Append fold losses to global losses\n",
    "        training_losses.extend(fold_training_losses)\n",
    "        val_losses.extend(fold_val_losses)\n",
    "\n",
    "    # Save the final model\n",
    "    final_model_path = os.path.join(output_folder, f\"{model_save_name}_final.pt\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"\\nModel saved as {final_model_path}\")\n",
    "\n",
    "    # Plot and save the training and validation losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    epochs_total = range(1, len(training_losses) + 1)\n",
    "    plt.plot(epochs_total, training_losses, label='Training Loss')\n",
    "    plt.plot(epochs_total, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'{model_save_name} Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    loss_plot_path = os.path.join(output_folder, f\"{model_save_name}_loss_plot.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4833cca7-d78d-4563-add3-9a6976e0a895",
   "metadata": {},
   "source": [
    "### Explanation of `testModel` Function\r\n",
    "\r\n",
    "The `testModel` function is designed to evaluate the Variational Autoencoder (VAE) model on a test dataset, measuring its performance in terms of reconstruction accuracy and perceptual quality. It computes the average test loss, Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM), which are critical metrics for assessing the quality of reconstructed images.\r\n",
    "\r\n",
    "The function starts by preparing the test dataset, loading images from the specified path, and applying the necessary transformations such as resizing and normalization. The model is then set to evaluation mode using `model.eval()`, ensuring that layers like dropout and batch normalization behave correctly. To optimize memory usage and speed during evaluation, gradient computation is disabled using the `torch.no_grad()` context.\r\n",
    "\r\n",
    "For each batch in the test dataset, the function calculates the test loss using the `VAELoss` function, which combines reconstruction and KL divergence terms. In addition to the loss, the function computes PSNR and SSIM metrics for each image, which evaluate the quality of reconstructed images compared to their ground truth. PSNR measures the signal fidelity, while SSIM assesses the perceptual similarity between images.\r\n",
    "\r\n",
    "After processing all batches, the function calculates the average test loss, PSNR, and SSIM values. These results are printed for immediate analysis and returned as a dictionary for further use, such as logging or comparison with other models. The `testModel` function provides a comprehensive evaluation framework, ensuring that the model's performance is measured accurately on unsen data.\r\n",
    "\n",
    "images.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ed2a380-ffeb-4462-987a-bc7361a2dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, test_path, transform, mini_batch_size, device, model_save_name, image_size):\n",
    "    \"\"\"\n",
    "    Test the VAE model on a test dataset and compute evaluation metrics.\n",
    "\n",
    "    This function evaluates the VAE model on the provided test dataset, calculates\n",
    "    the test loss using the custom loss function, and computes the PSNR and SSIM\n",
    "    metrics for the reconstructed images.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained VAE model.\n",
    "        test_path (str): Path to the test dataset.\n",
    "        transform (callable): Transformations to apply to the images.\n",
    "        mini_batch_size (int): Batch size for testing.\n",
    "        device (str): Device to run the testing on ('cuda' or 'cpu').\n",
    "        model_save_name (str): Name of the model (for display purposes).\n",
    "        image_size (int): Image size to use for the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the average test loss, PSNR, and SSIM.\n",
    "    \"\"\"\n",
    "    # Create the test dataset\n",
    "    dataset = WatermarkDataset(\n",
    "        csv_file_path=f\"{test_path}/metadata.csv\",\n",
    "        database_root_dir=test_path,\n",
    "        subsample=1,\n",
    "        image_size=image_size,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Create a DataLoader for the test dataset\n",
    "    test_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=mini_batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Define the custom loss function\n",
    "    loss_func = VAELoss()\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation for testing\n",
    "    with torch.no_grad():\n",
    "        test_losses = []  # List to store test losses\n",
    "        psnrs = []        # List to store PSNR values\n",
    "        ssims = []        # List to store SSIM values\n",
    "\n",
    "        # Iterate over the test dataset\n",
    "        for input_images, target_images in test_loader:\n",
    "            # Move data to the specified device\n",
    "            input_images = input_images.to(device)\n",
    "            target_images = target_images.to(device)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            recon_images, mu, logvar, predicted_masks = model(input_images)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = loss_func(recon_images, target_images, input_images, mu, logvar, predicted_masks, kl_factor=0.1)\n",
    "            test_losses.append(loss.item())\n",
    "\n",
    "            # Move tensors to CPU for metric calculation\n",
    "            recon_images_np = recon_images.permute(0, 2, 3, 1).cpu().numpy()\n",
    "            target_images_np = target_images.permute(0, 2, 3, 1).cpu().numpy()\n",
    "\n",
    "            target_images_np = target_images_np / 255.0\n",
    "            recon_images_np = recon_images_np / 255.0\n",
    "\n",
    "            # Calculate PSNR and SSIM for each image in the batch\n",
    "            for i in range(recon_images_np.shape[0]):\n",
    "                # PSNR Calculation\n",
    "                curr_mse = mse(target_images_np[i], recon_images_np[i])\n",
    "                if curr_mse == 0:\n",
    "                    cur_psnr = 1.0  # Completely identical images\n",
    "                else:\n",
    "                    cur_psnr = psnr(target_images_np[i], recon_images_np[i], data_range=1.0)\n",
    "                    \n",
    "                # SSIM Calculation\n",
    "                min_side = min(target_images_np[i].shape[:2])\n",
    "                win_size = min(min_side, 7)  # Dynamic win_size\n",
    "                if win_size % 2 == 0:\n",
    "                    win_size -= 1  # Ensure odd value for win_size\n",
    "        \n",
    "                data_range = target_images_np[i].max() - target_images_np[i].min()\n",
    "                if data_range == 0:\n",
    "                    data_range = 1e-5\n",
    "                    \n",
    "                cur_ssim = ssim(\n",
    "                    target_images_np[i],\n",
    "                    recon_images_np[i],\n",
    "                    win_size=win_size,\n",
    "                    data_range=data_range,\n",
    "                    multichannel=True,\n",
    "                    channel_axis=-1\n",
    "                )\n",
    "\n",
    "                psnrs.append(cur_psnr)\n",
    "                ssims.append(cur_ssim)\n",
    "\n",
    "        # Calculate average metrics\n",
    "        avg_test_loss = np.mean(test_losses)\n",
    "        avg_psnr = np.mean(psnrs)\n",
    "        avg_ssim = np.mean(ssims)\n",
    "\n",
    "        # Print results\n",
    "        print(f\"{model_save_name} Test Results:\")\n",
    "        print(f\"\\tAverage Test Loss: {avg_test_loss:.4f}\")\n",
    "        print(f\"\\tAverage PSNR: {avg_psnr:.2f} dB\")\n",
    "        print(f\"\\tAverage SSIM: {avg_ssim:.4f}\")\n",
    "\n",
    "        # Return the results as a dictionary\n",
    "        return {\n",
    "            \"test_loss\": avg_test_loss,\n",
    "            \"psnr\": avg_psnr,\n",
    "            \"ssim\": avg_ssim\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bad21e-bce9-4c20-b8a9-65063c24018b",
   "metadata": {},
   "source": [
    "### Explanation of `initModelWeights` Function\r\n",
    "\r\n",
    "The `initModelWeights` function is responsible for initializing the weights and biases of the layers in the VAE model, ensuring effective training by starting with well-initialized parameters. Proper weight initialization helps prevent issues such as vanishing or exploding gradients, which can hinder the learning process.\r\n",
    "\r\n",
    "The function applies specific initialization techniques tailored to different layer types in the model. For convolutional layers (`nn.Conv2d` and `nn.ConvTranspose2d`), it uses Kaiming Normal (He Initialization) to initialize the weights. This method is particularly suited for layers with ReLU activations, as it maintains a consistent variance of activations across layers. The biases for these layers are set to 0 to ensure no initial offset in the computations.\r\n",
    "\r\n",
    "For batch normalization layers (`nn.BatchNorm2d`), the scale parameter (weight) is initialized to 1, ensuring that the initial normalization preserves the scale of inputs. The shift parameter (bias) is set to 0, providing a neutral starting point for normalization adjustments.\r\n",
    "\r\n",
    "In the case of linear layers (`nn.Linear`), the function employs Xavier Normal initialization for weights. This technique is effective for layers with linear or sigmoid activations, as it maintains a balanced variance of activations across layers. Biases for linear layers are also initialized to 0, ensuring consistency across different layer types.\r\n",
    "\r\n",
    "By handling multiple layer types and applying the most suitable initialization techniques, the `initModelWeights` function provides flexible and robust weight initialization. This flexibility not only stabilizes gradients during training but also ensures that the model starts with parameters optimized for efficientlearning.\r\n",
    "\n",
    ".\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bc785f9-9cb5-44a7-90be-f17f81af7898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initModelWeights(m):\n",
    "    \"\"\"\n",
    "    Initialize the weights of the model layers.\n",
    "\n",
    "    This function initializes the weights of convolutional, batch normalization,\n",
    "    and linear layers in the VAE model using appropriate initialization techniques.\n",
    "    Proper weight initialization helps in faster convergence and avoids issues like\n",
    "    vanishing or exploding gradients.\n",
    "\n",
    "    Args:\n",
    "        m (torch.nn.Module): A module in the VAE model whose weights need to be initialized.\n",
    "\n",
    "    Returns:\n",
    "        None: Modifies the weights of the module in place.\n",
    "    \"\"\"\n",
    "    # Check if the module is a 2D convolutional layer or a transposed convolutional layer\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        # Initialize weights using Kaiming Normal initialization (He initialization)\n",
    "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "\n",
    "        # Initialize biases to 0 if they exist\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # Check if the module is a 2D batch normalization layer\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        # Initialize the scale parameter (weight) to 1\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "\n",
    "        # Initialize the shift parameter (bias) to 0\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    # Check if the module is a linear (fully connected) layer\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # Initialize weights using Xavier Normal initialization\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "        # Initialize biases to 0\n",
    "        nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b7d13-97ec-48c4-a442-c7e78ec5a84b",
   "metadata": {},
   "source": [
    "### Explanation of `startTraining` Function\r\n",
    "\r\n",
    "The `startTraining` function orchestrates the complete training and testing workflow for the Variational Autoencoder (VAE) model. It utilizes the best hyperparameters obtained from a previous hyperparameter tuning process to train the model on a training dataset and evaluate its performance on a test dataset. This function ensures an efficient and systematic approach to model development and validation.\r\n",
    "\r\n",
    "The function begins by extracting hyperparameters such as batch size, learning rate, number of epochs, KL divergence weight, and mask weight from the provided `best_config` dictionary. These parameters are crucial for controlling various aspects of training and regularization. \r\n",
    "\r\n",
    "The training dataset is then prepared by loading it from the specified path and applying necessary transformations, such as resizing and normalization, to standardize the data. The model is initialized with the specified latent dimension and image size, followed by applying a custom weight initialization function to set up the model with well-initialized weights, ensuring stable and efficient training.\r\n",
    "\r\n",
    "For training, the function leverages the `trainModel` function, which employs K-Fold Cross Validation to train and validate the model across multiple folds. This ensures robust performance evaluation and reduces the risk of overfitting. Once training is complete, the model is tested on the test dataset using the `testModel` function, which computes key evaluation metrics such as test loss, Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM). These metrics provide a comprehensive assessment of the model's reconstruction quality and overall performance.\r\n",
    "\r\n",
    "Finally, the function performs resource cleanup by deleting the model instance and clearing the GPU cache (if applicable) to free up memory for subsequent tasks. The `startTraining` function integrates all essential steps in the model development pipeline, from training to testing, ensuring a seamless workflow for VAE-based watermark remoal tasks.\r\n",
    "ettings.\r\n",
    "erations.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e63cc59-206e-4a6d-b8ab-cfd0feca873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def startTraining(best_config, transform, initModelWeights, image_size, num_folds, train_path, test_path, device, yolo_model_path, model_save_name):\n",
    "    \"\"\"\n",
    "    Start the training and testing process for the VAE model.\n",
    "\n",
    "    This function trains the Variational Autoencoder (VAE) model using the best hyperparameter\n",
    "    configuration found during hyperparameter tuning. It initializes the model, applies\n",
    "    weight initialization, trains it using K-Fold Cross Validation, and evaluates it\n",
    "    on a test dataset.\n",
    "\n",
    "    Args:\n",
    "        best_config (dict): Dictionary containing the best hyperparameters from hyperparameter tuning.\n",
    "        transform (callable): Transformations to apply to the dataset (e.g., resizing, normalization).\n",
    "        initModelWeights (callable): Function to initialize the model weights.\n",
    "        image_size (int): Size of the input images (height and width).\n",
    "        num_folds (int): Number of folds for K-Fold Cross Validation.\n",
    "        train_path (str): Path to the training dataset.\n",
    "        test_path (str): Path to the test dataset.\n",
    "        device (str): Device to run the training on ('cuda' or 'cpu').\n",
    "        model_save_name (str): Name for saving the trained model and outputs.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Extract hyperparameters from the best configuration\n",
    "    mini_batch_size = best_config[\"batch_size\"]\n",
    "    learning_rate = best_config[\"lr\"]\n",
    "    n_epochs = best_config[\"n_epochs\"]\n",
    "    latent_dim = best_config[\"latent_dim\"]\n",
    "\n",
    "    # Create the training dataset\n",
    "    dataset = WatermarkDataset(\n",
    "        csv_file_path=os.path.join(train_path, \"metadata.csv\"),\n",
    "        database_root_dir=train_path,\n",
    "        image_size=image_size,\n",
    "        subsample=1,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # Initialize the VAE model and move it to the specified device\n",
    "    model = VAE(\n",
    "        in_channels=4, \n",
    "        out_channels=3, \n",
    "        latent_dim=latent_dim, \n",
    "        input_size=image_size,\n",
    "        yolo_model_path=yolo_model_path,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "\n",
    "    # Apply weight initialization to the model\n",
    "    model.apply(initModelWeights)\n",
    "\n",
    "    # Set up the optimizer (Adam) with the specified learning rate\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    # Train the model using K-Fold Cross Validation\n",
    "    trainModel(\n",
    "        model=model,\n",
    "        optim=optim,\n",
    "        dataset=dataset,\n",
    "        n_epochs=n_epochs,\n",
    "        mini_batch_size=mini_batch_size,\n",
    "        num_folds=num_folds,\n",
    "        device=device,\n",
    "        model_save_name=model_save_name\n",
    "    )\n",
    "\n",
    "    # Test the model on the test dataset\n",
    "    testModel(\n",
    "        model=model,\n",
    "        test_path=test_path,\n",
    "        transform=transform,\n",
    "        mini_batch_size=mini_batch_size,\n",
    "        device=device,\n",
    "        model_save_name=model_save_name,\n",
    "        image_size=image_size\n",
    "    )\n",
    "\n",
    "    # Free up memory by deleting the model and clearing the GPU cache (if applicable)\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c7d84-fb85-4dc8-a5fd-b6df29ac68db",
   "metadata": {},
   "source": [
    "### Definitions\r\n",
    "\r\n",
    "The provided code block sets up the device and training configuration for the model.\r\n",
    "\r\n",
    "First, it checks if CUDA (GPU support) is available. If a GPU is detected, the name of the GPU is printed for confirmation; otherwise, a message is displayed indicating that no GPU is found, meaning the model will run on the CPU, which may result in slower processing. Based on the availability of CUDA, the `device` is set to either GPU or CPU, ensuring compatibility with the hardware.\r\n",
    "\r\n",
    "The image size for the dataset is defined as 128 pixels, which determines the dimensions to which all input images will be resized. The number of folds for K-Fold Cross Validation is set to 5, allowing the model to be validated on multiple splits of the dataset, thereby improving its robustness and generalization.\r\n",
    "\r\n",
    "To preprocess the images, a transformation pipeline is defined. The pipeline includes:\r\n",
    "- **`Resize`**: Resizes the input images to a uniform size of 128x128 pixels, ensuring consistency in model input dimensions.\r\n",
    "- **`ToTensor`**: Converts images into PyTorch tensors, a format compatible with the model.\r\n",
    "\r\n",
    "These transformations are applied sequentially to ensure that the input data is properly formatted for training. Additionally, the transformations are logged to provide visibility into the preprocessing steps applied to the dataset, which aids in debugging and understanding the data preparaton process.\r\n",
    "et.\r\n",
    "aining.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d26b3096-e86c-4d93-8760-88d0daae7b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n",
      "Applying the following transformations: Compose(\n",
      "    Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=True)\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA availability and set the device\n",
    "if torch.cuda.is_available():\n",
    "    # Print the name of the GPU if available\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    # Fallback to CPU if no GPU is available\n",
    "    print(\"No GPU found. The model will run on CPU, which may be slower.\")\n",
    "\n",
    "# Set the device to CUDA if available, otherwise default to CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the image size for resizing input images in the dataset\n",
    "image_size = 128\n",
    "\n",
    "# Define the number of folds for K-Fold Cross Validation\n",
    "num_folds = 5\n",
    "\n",
    "# Define YOLO best model path\n",
    "base_path = os.getcwd()  # alma dizini\n",
    "yolo_model_path = os.path.join(base_path, \"yolo_best_model\", \"best.pt\")\n",
    "\n",
    "best_config = {\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 1e-3,\n",
    "    \"n_epochs\": 10,\n",
    "    \"latent_dim\": 256\n",
    "}\n",
    "\n",
    "# Define transformations for preprocessing images\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),  # Resize images to the defined size\n",
    "        transforms.ToTensor()                        # Convert images to PyTorch tensors\n",
    "    ])\n",
    "\n",
    "# Log the transformations being applied for debugging purposes\n",
    "print(f\"Applying the following transformations: {transform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fe1c78-fe45-44eb-a61a-97804280b584",
   "metadata": {},
   "source": [
    "### No Logo And Low Opacity Model: Hyperparameter Optimization, Training, and Testing\r\n",
    "\r\n",
    "This code block outlines the comprehensive workflow for training the **No Logo Low Opacity VAE Model**, encompassing hyperparameter optimization, training, and testing.\r\n",
    "\r\n",
    "The process begins by defining the paths for the training and test datasets. These paths are constructed by appending the respective dataset folder names to the current working directory. To ensure the integrity of the workflow, the script checks for the existence of these directories. If any of the specified paths are missing, an error is raised, preventing further execution.\r\n",
    "\r\n",
    "Hyperparameter tuning is then initiated using the `startHyperparameterTuning` function. This function leverages the Hyperband scheduler with Ray Tune to explore various combinations of hyperparameters, including learning rate, batch size, KL divergence weight, latent dimension, and number of epochs. The key inputs to this function are:\r\n",
    "- A transformation pipeline (`transform`) for preprocessing images.\r\n",
    "- A weight initialization function (`initModelWeights`) to ensure proper model setup.\r\n",
    "- The training dataset path (`train_path`) for data preparation.\r\n",
    "- The image size (`image_size`) for resizing inputs.\r\n",
    "- The computation device (`device`), which is either GPU or CPU, depending on availability.\r\n",
    "\r\n",
    "The hyperparameter optimization process identifies the best configuration, which is then logged and used for model training. Once the optimal parameters are determined, the `startTraining` function is called to train the model using K-Fold Cross Validation. This robust training approach ensures that the model is validated across multiple dataset splits, enhancing its generalization and reducing the risk of overfitting.\r\n",
    "\r\n",
    "During training, the model's performance is evaluated using metrics such as reconstruction loss, Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index Measure (SSIM). These metrics provide insights into the quality of the reconstructed images and the overall effectiveness of the model.\r\n",
    "\r\n",
    "After training, the model is tested on a separate test dataset to assess its performance on unseen data. The trained model and its associated results are saved with a descriptive name that includes details such as the image size and number of folds used in cross-validation. This ensures that the model can be easily identified and reused for future tasks.\r\n",
    "\r\n",
    "Finally, the script confirms the successful completion of the hyperparameter optimization, training, and testing workflow, providing a comprehensive solution for watermark removal tasks wth the VAE model.\r\n",
    "ning and testing.\r\n",
    "\r\n",
    "### ***IMPORTANT NOTE***\r\n",
    "During hyperparameter tuning with Ray Tune, some warnings related to performance bottlenecks may be encountered. While these warnings do not prevent the process from completing, they suggest potential areas for optimizing the tuning workflow to improe efficiecy and scalability.\r\n",
    "is needed.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fd8ba55-c668-478a-a7dc-67d0ca5edca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning on cuda...\n",
      "Starting training with the best configuration...\n",
      "Training on device: cuda\n",
      "\n",
      "VAE_NoLogoLowOpacity_128px_5folds Fold 1/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.042826\n",
      "\t\tValidation Loss: 0.015227\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 503.681023\n",
      "\t\tValidation Loss: 125.167428\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 406.553346\n",
      "\t\tValidation Loss: 192.531059\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 380.057882\n",
      "\t\tValidation Loss: 253.740759\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 523.751203\n",
      "\t\tValidation Loss: 2.647768\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 0.473824\n",
      "\t\tValidation Loss: 0.851245\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 0.213039\n",
      "\t\tValidation Loss: 0.617133\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 0.063886\n",
      "\t\tValidation Loss: 0.520381\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 0.058703\n",
      "\t\tValidation Loss: 0.548439\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 0.056998\n",
      "\t\tValidation Loss: 0.840445\n",
      "\n",
      "VAE_NoLogoLowOpacity_128px_5folds Fold 2/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.016444\n",
      "\t\tValidation Loss: 0.012533\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 0.026374\n",
      "\t\tValidation Loss: 0.101706\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 0.036212\n",
      "\t\tValidation Loss: 0.125750\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 0.040441\n",
      "\t\tValidation Loss: 0.237107\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 0.041658\n",
      "\t\tValidation Loss: 0.125811\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 0.038809\n",
      "\t\tValidation Loss: 0.144649\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 0.034652\n",
      "\t\tValidation Loss: 0.089695\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 0.022821\n",
      "\t\tValidation Loss: 0.126866\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 0.022843\n",
      "\t\tValidation Loss: 0.037532\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 0.021474\n",
      "\t\tValidation Loss: 0.168295\n",
      "\n",
      "VAE_NoLogoLowOpacity_128px_5folds Fold 3/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.013418\n",
      "\t\tValidation Loss: 0.011124\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 0.015667\n",
      "\t\tValidation Loss: 0.075682\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 0.017699\n",
      "\t\tValidation Loss: 0.059938\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 0.020090\n",
      "\t\tValidation Loss: 0.104371\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 0.020747\n",
      "\t\tValidation Loss: 0.154961\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 0.022019\n",
      "\t\tValidation Loss: 0.033306\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 0.022676\n",
      "\t\tValidation Loss: 0.106025\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 0.018308\n",
      "\t\tValidation Loss: 0.124322\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 0.017772\n",
      "\t\tValidation Loss: 0.065120\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 0.017815\n",
      "\t\tValidation Loss: 0.126497\n",
      "\n",
      "VAE_NoLogoLowOpacity_128px_5folds Fold 4/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.012950\n",
      "\t\tValidation Loss: 0.010163\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 0.014388\n",
      "\t\tValidation Loss: 0.014510\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 0.015735\n",
      "\t\tValidation Loss: 0.036780\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 0.017091\n",
      "\t\tValidation Loss: 0.044969\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 0.018488\n",
      "\t\tValidation Loss: 0.027341\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 0.019803\n",
      "\t\tValidation Loss: 0.071209\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 0.021453\n",
      "\t\tValidation Loss: 0.144342\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 0.017574\n",
      "\t\tValidation Loss: 0.037058\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 0.017471\n",
      "\t\tValidation Loss: 0.056774\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 0.017407\n",
      "\t\tValidation Loss: 0.025329\n",
      "\n",
      "VAE_NoLogoLowOpacity_128px_5folds Fold 5/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.012787\n",
      "\t\tValidation Loss: 0.009901\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 0.014071\n",
      "\t\tValidation Loss: 0.014853\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 0.015336\n",
      "\t\tValidation Loss: 0.015733\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 0.016670\n",
      "\t\tValidation Loss: 0.021157\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 0.018142\n",
      "\t\tValidation Loss: 0.045669\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 0.019397\n",
      "\t\tValidation Loss: 0.021976\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 0.020696\n",
      "\t\tValidation Loss: 0.020658\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 0.017503\n",
      "\t\tValidation Loss: 0.017402\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 0.017565\n",
      "\t\tValidation Loss: 0.029964\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 0.017314\n",
      "\t\tValidation Loss: 0.039228\n",
      "\n",
      "Model saved as outputs\\VAE_NoLogoLowOpacity_128px_5folds_final.pt\n",
      "VAE_NoLogoLowOpacity_128px_5folds Test Results:\n",
      "\tAverage Test Loss: 0.0208\n",
      "\tAverage PSNR: 98.92 dB\n",
      "\tAverage SSIM: 0.9943\n",
      "Training and testing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the paths for the training and test datasets\n",
    "train_path = os.path.join(os.getcwd(), \"no_logo_and_low_opacity_watermark_dataset_train\")\n",
    "test_path = os.path.join(os.getcwd(), \"no_logo_and_low_opacity_watermark_dataset_test\")\n",
    "\n",
    "# Ensure the dataset paths exist\n",
    "if not os.path.exists(train_path):\n",
    "    raise FileNotFoundError(f\"Training dataset path not found: {train_path}\")\n",
    "if not os.path.exists(test_path):\n",
    "    raise FileNotFoundError(f\"Test dataset path not found: {test_path}\")\n",
    "\n",
    "# Start the hyperparameter tuning process\n",
    "print(f\"Starting hyperparameter tuning on {device}...\")\n",
    "\n",
    "# Define a descriptive model save name\n",
    "model_save_name = f\"VAE_NoLogoLowOpacity_{image_size}px_{num_folds}folds\"\n",
    "\n",
    "# Start the training process with the best configuration\n",
    "print(\"Starting training with the best configuration...\")\n",
    "startTraining(\n",
    "    best_config=best_config,\n",
    "    transform=transform,\n",
    "    initModelWeights=initModelWeights,\n",
    "    image_size=image_size,\n",
    "    num_folds=num_folds,\n",
    "    train_path=train_path,\n",
    "    test_path=test_path,\n",
    "    device=device,\n",
    "    yolo_model_path=yolo_model_path,\n",
    "    model_save_name=model_save_name\n",
    ")\n",
    "\n",
    "print(\"Training and testing completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5f0f49-6739-4930-8e27-21816ff529a0",
   "metadata": {},
   "source": [
    "### No Logo And High Opacity Model: Hyperparameter Optimization, Training And Test\n",
    "Below block creates No Logo And High Opacity VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e742b2ca-4063-4a64-aedf-ccf7c558a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning on cuda...\n",
      "Starting training with the best configuration...\n",
      "Training on device: cuda\n",
      "\n",
      "VAE_NoLogoHighOpacity_128px_5folds Fold 1/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.066940\n",
      "\t\tValidation Loss: 0.037901\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 563.287345\n",
      "\t\tValidation Loss: 82.630151\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 302.788562\n",
      "\t\tValidation Loss: 188.976936\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 493.418668\n",
      "\t\tValidation Loss: 564.901319\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 550.126229\n",
      "\t\tValidation Loss: 194.641072\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 615.803178\n",
      "\t\tValidation Loss: 207.624019\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 690.096535\n",
      "\t\tValidation Loss: 250.069527\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 406.334134\n",
      "\t\tValidation Loss: 218.396873\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 377.052106\n",
      "\t\tValidation Loss: 245.135470\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 334.427801\n",
      "\t\tValidation Loss: 241.538370\n",
      "\n",
      "VAE_NoLogoHighOpacity_128px_5folds Fold 2/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.040125\n",
      "\t\tValidation Loss: 0.037932\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 91.100378\n",
      "\t\tValidation Loss: 58.358258\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 170.436042\n",
      "\t\tValidation Loss: 106.326269\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 241.767950\n",
      "\t\tValidation Loss: 174.278871\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 312.555942\n",
      "\t\tValidation Loss: 235.574290\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 385.927498\n",
      "\t\tValidation Loss: 251.195623\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 458.768627\n",
      "\t\tValidation Loss: 333.877376\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 268.667642\n",
      "\t\tValidation Loss: 144.457822\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 265.697423\n",
      "\t\tValidation Loss: 147.926980\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 263.332594\n",
      "\t\tValidation Loss: 165.988076\n",
      "\n",
      "VAE_NoLogoHighOpacity_128px_5folds Fold 3/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.035483\n",
      "\t\tValidation Loss: 0.032529\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 73.904062\n",
      "\t\tValidation Loss: 44.864217\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 151.379906\n",
      "\t\tValidation Loss: 88.002851\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 220.025614\n",
      "\t\tValidation Loss: 101.162811\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 304.499595\n",
      "\t\tValidation Loss: 297.004042\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 378.403460\n",
      "\t\tValidation Loss: 222.172415\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 455.179612\n",
      "\t\tValidation Loss: 312.024001\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 262.784056\n",
      "\t\tValidation Loss: 168.461043\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 263.433666\n",
      "\t\tValidation Loss: 151.439628\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 267.467574\n",
      "\t\tValidation Loss: 214.027065\n",
      "\n",
      "VAE_NoLogoHighOpacity_128px_5folds Fold 4/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.035452\n",
      "\t\tValidation Loss: 0.032428\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 74.728436\n",
      "\t\tValidation Loss: 36.250123\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 149.694388\n",
      "\t\tValidation Loss: 92.693498\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 218.806583\n",
      "\t\tValidation Loss: 155.260090\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 295.518137\n",
      "\t\tValidation Loss: 187.793229\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 369.677788\n",
      "\t\tValidation Loss: 265.012964\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 449.934799\n",
      "\t\tValidation Loss: 214.031227\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 255.332235\n",
      "\t\tValidation Loss: 124.881048\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 263.481412\n",
      "\t\tValidation Loss: 145.437225\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 257.453890\n",
      "\t\tValidation Loss: 113.684467\n",
      "\n",
      "VAE_NoLogoHighOpacity_128px_5folds Fold 5/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.035359\n",
      "\t\tValidation Loss: 0.032538\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 74.353578\n",
      "\t\tValidation Loss: 38.835971\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 149.900173\n",
      "\t\tValidation Loss: 86.803639\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 222.508768\n",
      "\t\tValidation Loss: 179.653619\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 298.234357\n",
      "\t\tValidation Loss: 192.525142\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 376.340372\n",
      "\t\tValidation Loss: 239.783174\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 454.836885\n",
      "\t\tValidation Loss: 278.357040\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 260.763058\n",
      "\t\tValidation Loss: 156.868235\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 259.697170\n",
      "\t\tValidation Loss: 213.395473\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 270.533633\n",
      "\t\tValidation Loss: 207.279912\n",
      "\n",
      "Model saved as outputs\\VAE_NoLogoHighOpacity_128px_5folds_final.pt\n",
      "VAE_NoLogoHighOpacity_128px_5folds Test Results:\n",
      "\tAverage Test Loss: 10.8128\n",
      "\tAverage PSNR: 92.46 dB\n",
      "\tAverage SSIM: 0.9882\n",
      "Training and testing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the paths for the training and test datasets\n",
    "train_path = os.getcwd() + \"/no_logo_and_high_opacity_watermark_dataset_train\"\n",
    "test_path = os.getcwd() + \"/no_logo_and_high_opacity_watermark_dataset_test\"\n",
    "\n",
    "# Ensure the dataset paths exist\n",
    "if not os.path.exists(train_path):\n",
    "    raise FileNotFoundError(f\"Training dataset path not found: {train_path}\")\n",
    "if not os.path.exists(test_path):\n",
    "    raise FileNotFoundError(f\"Test dataset path not found: {test_path}\")\n",
    "\n",
    "# Start the hyperparameter tuning process\n",
    "print(f\"Starting hyperparameter tuning on {device}...\")\n",
    "\n",
    "# Define a descriptive model save name\n",
    "model_save_name = f\"VAE_NoLogoHighOpacity_{image_size}px_{num_folds}folds\"\n",
    "\n",
    "# Start the training process with the best configuration\n",
    "print(\"Starting training with the best configuration...\")\n",
    "startTraining(\n",
    "    best_config=best_config,\n",
    "    transform=transform,\n",
    "    initModelWeights=initModelWeights,\n",
    "    image_size=image_size,\n",
    "    num_folds=num_folds,\n",
    "    train_path=train_path,\n",
    "    test_path=test_path,\n",
    "    device=device,\n",
    "    yolo_model_path=yolo_model_path,\n",
    "    model_save_name=model_save_name\n",
    ")\n",
    "\n",
    "print(\"Training and testing completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b449353-632b-4c47-b0ab-86af120ee6e0",
   "metadata": {},
   "source": [
    "### Logo And High Opacity Model: Hyperparameter Optimization, Training And Test\n",
    "Below block creates Logo And High Opacity VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4a5882c-2a59-461d-9881-142cf684c14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning on cuda...\n",
      "Starting training with the best configuration...\n",
      "Training on device: cuda\n",
      "\n",
      "VAE_LogoHighOpacity_128px_5folds Fold 1/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.148569\n",
      "\t\tValidation Loss: 0.087056\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 1332.320631\n",
      "\t\tValidation Loss: 55.069904\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 315.469157\n",
      "\t\tValidation Loss: 252.460368\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 425.071306\n",
      "\t\tValidation Loss: 373.795685\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 261.383133\n",
      "\t\tValidation Loss: 4.742695\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 0.529162\n",
      "\t\tValidation Loss: 0.621556\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 0.207183\n",
      "\t\tValidation Loss: 0.264693\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 0.121647\n",
      "\t\tValidation Loss: 0.151179\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 0.118230\n",
      "\t\tValidation Loss: 0.650892\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 0.109631\n",
      "\t\tValidation Loss: 0.142096\n",
      "\n",
      "VAE_LogoHighOpacity_128px_5folds Fold 2/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.075088\n",
      "\t\tValidation Loss: 0.068638\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 0.081743\n",
      "\t\tValidation Loss: 0.132576\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 0.086724\n",
      "\t\tValidation Loss: 0.130516\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 0.088967\n",
      "\t\tValidation Loss: 1.169897\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 0.086413\n",
      "\t\tValidation Loss: 0.294748\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 0.078788\n",
      "\t\tValidation Loss: 0.233829\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 0.073591\n",
      "\t\tValidation Loss: 0.633817\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 0.065945\n",
      "\t\tValidation Loss: 0.154427\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 0.065297\n",
      "\t\tValidation Loss: 0.107407\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 0.065721\n",
      "\t\tValidation Loss: 0.075252\n",
      "\n",
      "VAE_LogoHighOpacity_128px_5folds Fold 3/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.063013\n",
      "\t\tValidation Loss: 0.061581\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 0.063455\n",
      "\t\tValidation Loss: 0.062588\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 0.063593\n",
      "\t\tValidation Loss: 0.069488\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 0.063478\n",
      "\t\tValidation Loss: 0.084075\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 0.063379\n",
      "\t\tValidation Loss: 0.067576\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 0.062616\n",
      "\t\tValidation Loss: 0.064824\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 0.062755\n",
      "\t\tValidation Loss: 0.078679\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 0.061176\n",
      "\t\tValidation Loss: 0.062757\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 0.061277\n",
      "\t\tValidation Loss: 0.069481\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 0.060873\n",
      "\t\tValidation Loss: 0.062817\n",
      "\n",
      "VAE_LogoHighOpacity_128px_5folds Fold 4/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.062098\n",
      "\t\tValidation Loss: 0.054806\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 0.061963\n",
      "\t\tValidation Loss: 0.055978\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 0.062174\n",
      "\t\tValidation Loss: 0.062128\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 0.062312\n",
      "\t\tValidation Loss: 0.081586\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 0.062282\n",
      "\t\tValidation Loss: 0.059247\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 0.062510\n",
      "\t\tValidation Loss: 0.064028\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 0.063033\n",
      "\t\tValidation Loss: 0.056559\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 0.061791\n",
      "\t\tValidation Loss: 0.056363\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 0.062629\n",
      "\t\tValidation Loss: 0.063528\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 0.061937\n",
      "\t\tValidation Loss: 0.061599\n",
      "\n",
      "VAE_LogoHighOpacity_128px_5folds Fold 5/5\n",
      "\tEpoch 1/10\n",
      "\t\tTraining Loss: 0.060926\n",
      "\t\tValidation Loss: 0.055499\n",
      "\tEpoch 2/10\n",
      "\t\tTraining Loss: 0.061111\n",
      "\t\tValidation Loss: 0.056228\n",
      "\tEpoch 3/10\n",
      "\t\tTraining Loss: 0.061432\n",
      "\t\tValidation Loss: 0.060138\n",
      "\tEpoch 4/10\n",
      "\t\tTraining Loss: 0.061712\n",
      "\t\tValidation Loss: 0.060528\n",
      "\tEpoch 5/10\n",
      "\t\tTraining Loss: 0.061524\n",
      "\t\tValidation Loss: 0.058106\n",
      "\tEpoch 6/10\n",
      "\t\tTraining Loss: 0.061917\n",
      "\t\tValidation Loss: 0.061074\n",
      "\tEpoch 7/10\n",
      "\t\tTraining Loss: 0.062338\n",
      "\t\tValidation Loss: 0.059578\n",
      "\tEpoch 8/10\n",
      "\t\tTraining Loss: 0.061526\n",
      "\t\tValidation Loss: 0.064785\n",
      "\tEpoch 9/10\n",
      "\t\tTraining Loss: 0.061974\n",
      "\t\tValidation Loss: 0.059480\n",
      "\tEpoch 10/10\n",
      "\t\tTraining Loss: 0.061744\n",
      "\t\tValidation Loss: 0.058104\n",
      "\n",
      "Model saved as outputs\\VAE_LogoHighOpacity_128px_5folds_final.pt\n",
      "VAE_LogoHighOpacity_128px_5folds Test Results:\n",
      "\tAverage Test Loss: 0.0528\n",
      "\tAverage PSNR: 92.42 dB\n",
      "\tAverage SSIM: 0.9905\n",
      "Training and testing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the paths for the training and test datasets\n",
    "train_path = os.getcwd() + \"/logo_and_high_opacity_watermark_dataset_train\"\n",
    "test_path = os.getcwd() + \"/logo_and_high_opacity_watermark_dataset_test\"\n",
    "\n",
    "# Ensure the dataset paths exist\n",
    "if not os.path.exists(train_path):\n",
    "    raise FileNotFoundError(f\"Training dataset path not found: {train_path}\")\n",
    "if not os.path.exists(test_path):\n",
    "    raise FileNotFoundError(f\"Test dataset path not found: {test_path}\")\n",
    "\n",
    "# Start the hyperparameter tuning process\n",
    "print(f\"Starting hyperparameter tuning on {device}...\")\n",
    "\n",
    "# Define a descriptive model save name\n",
    "model_save_name = f\"VAE_LogoHighOpacity_{image_size}px_{num_folds}folds\"\n",
    "\n",
    "# Start the training process with the best configuration\n",
    "print(\"Starting training with the best configuration...\")\n",
    "startTraining(\n",
    "    best_config=best_config,\n",
    "    transform=transform,\n",
    "    initModelWeights=initModelWeights,\n",
    "    image_size=image_size,\n",
    "    num_folds=num_folds,\n",
    "    train_path=train_path,\n",
    "    test_path=test_path,\n",
    "    device=device,\n",
    "    yolo_model_path=yolo_model_path,   model_save_name=model_save_name\n",
    ")\n",
    "\n",
    "print(\"Training and testing completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-example]",
   "language": "python",
   "name": "conda-env-.conda-example-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
