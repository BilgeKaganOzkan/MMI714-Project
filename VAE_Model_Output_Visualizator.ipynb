{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4806706-6435-438e-a643-5dcbcbc99d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e15e0f-b2ea-4cc6-aa33-2fa5f43b17cf",
   "metadata": {},
   "source": [
    "### Explanation of DoubleConv, Down, Up, OutConv, and VAE Classes\n",
    "\n",
    "The `DoubleConv`, `Down`, `Up`, and `OutConv` classes serve as fundamental components in constructing the encoder-decoder architecture utilized in the `VAE` class. These modules facilitate various stages of feature extraction, downsampling, upsampling, and output generation, forming the backbone of the Variational Autoencoder (VAE) designed specifically for watermark removal.\n",
    "\n",
    "- **DoubleConv** acts as a core building block that sequentially applies two convolutional layers, each followed by Batch Normalization and ReLU activation. This ensures effective feature extraction with normalization for stable training.\n",
    "- **Down** performs downsampling using a MaxPooling layer, followed by a `DoubleConv` module for extracting hierarchical features while reducing spatial dimensions.\n",
    "- **Up** focuses on upsampling the feature maps using a transposed convolution, followed by concatenation with skip connections from the encoder. It then applies a `DoubleConv` to refine the upsampled features, enabling high-resolution reconstruction.\n",
    "- **OutConv** is responsible for producing the final output of the model by reducing the number of channels and applying a Sigmoid activation. This is particularly suited for pixel-level predictions, such as mask generation or image reconstruction.\n",
    "- **VAE** integrates these components within an encoder-decoder framework and incorporates a latent space for compact representation. Additionally, it leverages a pre-trained YOLO model to identify watermark regions, using these predictions as an extra input channel to enhance the reconstruction process in masked areas.\n",
    "\n",
    "The VAE model operates through three primary stages: detecting watermarked regions via YOLO, encoding the input image into a latent representation, and decoding this representation to reconstruct a watermark-free image. This modular design, combining feature extraction, downsampling, and upsampling with guidance from YOLO, enables precise and efficient watermar removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4b23ab-851f-4da3-b798-5634d64e7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Define the DoubleConv module: Two convolutional layers with BatchNorm and ReLU\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A sequential module consisting of two convolutional layers, \n",
    "    each followed by BatchNorm and ReLU activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the DoubleConv module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DoubleConv module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after two convolutions.\n",
    "        \"\"\"\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the Down module: MaxPooling followed by DoubleConv\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"\n",
    "    A module for downsampling that applies MaxPooling followed by a DoubleConv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the Down module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),  # Downsample by a factor of 2\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Down module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Downsampled tensor after MaxPooling and DoubleConv.\n",
    "        \"\"\"\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the Up module: Upsampling followed by DoubleConv\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"\n",
    "    A module for upsampling that applies ConvTranspose2d for resizing, \n",
    "    concatenates with skip connections, and applies DoubleConv.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the Up module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super(Up, self).__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Forward pass of the Up module.\n",
    "\n",
    "        Args:\n",
    "            x1 (torch.Tensor): Tensor from the decoder path (upsampled).\n",
    "            x2 (torch.Tensor): Tensor from the encoder path (skip connection).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after upsampling and concatenation.\n",
    "        \"\"\"\n",
    "        # Perform upsampling\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # Handle size mismatch due to rounding in upsampling\n",
    "        diffY = x2.size(2) - x1.size(2)\n",
    "        diffX = x2.size(3) - x1.size(3)\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        # Concatenate and apply DoubleConv\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the OutConv module: Final output layer with Sigmoid activation\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A module for reducing the number of channels to the desired output \n",
    "    and applying Sigmoid activation for pixel-wise predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Initialize the OutConv module.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "        \"\"\"\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),  # 1x1 convolution\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the OutConv module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (B, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with reduced channels.\n",
    "        \"\"\"\n",
    "        return self.conv(x)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Define the VAE (Variational Autoencoder) Model\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom Variational Autoencoder (VAE) model for watermark removal. \n",
    "    Integrates a YOLO object detection model for mask prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=4, out_channels=3, latent_dim=256, input_size=128, yolo_model_path=\"yolov8s.pt\", device='cuda'):\n",
    "        \"\"\"\n",
    "        Initialize the VAE model.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels (e.g., 4 for RGB + Mask).\n",
    "            out_channels (int): Number of output channels (e.g., 3 for RGB).\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "            input_size (int): Size of the input image (height and width).\n",
    "            yolo_model_path (str): Path to the pre-trained YOLO model.\n",
    "            device (str): Device to run the model on ('cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        yolo = YOLO(yolo_model_path)\n",
    "        object.__setattr__(self, 'yolo_model', yolo)\n",
    "        self.device = device\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Encoder layers\n",
    "        self.inc = DoubleConv(in_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(1024 * (input_size // 16) ** 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(1024 * (input_size // 16) ** 2, latent_dim)\n",
    "        self.fc_dec = nn.Linear(latent_dim, 1024 * (input_size // 16) ** 2)\n",
    "\n",
    "        # Decoder layers\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = OutConv(64, out_channels)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick for the latent space.\n",
    "\n",
    "        Args:\n",
    "            mu (torch.Tensor): Mean of the latent distribution.\n",
    "            logvar (torch.Tensor): Log variance of the latent distribution.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Sampled latent vector.\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def create_mask_from_yolo_preds(self, predictions, image_size):\n",
    "        \"\"\"\n",
    "        Create a binary mask from YOLO predictions.\n",
    "\n",
    "        Args:\n",
    "            predictions: YOLO detection results.\n",
    "            image_size (int): Size of the output mask (H, W).\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Binary mask of shape (1, H, W).\n",
    "        \"\"\"\n",
    "        mask = torch.zeros((1, image_size, image_size))\n",
    "        for det in predictions:\n",
    "            boxes = det.boxes\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                mask[:, y1:y2, x1:x2] = 1.0\n",
    "        return mask\n",
    "\n",
    "    def forward(self, input_images):\n",
    "        \"\"\"\n",
    "        Forward pass of the VAE model.\n",
    "\n",
    "        Args:\n",
    "            input_images (torch.Tensor): Batch of input images.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (Final output, mean, log variance, predicted masks).\n",
    "        \"\"\"\n",
    "        # YOLO mask prediction\n",
    "        batch_size = input_images.size(0)\n",
    "        predicted_masks = []\n",
    "        for i in range(batch_size):\n",
    "            img_np = (input_images[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "            results = self.yolo_model.predict(img_np, imgsz=self.input_size, verbose=False, device=self.device)\n",
    "            pmask = self.create_mask_from_yolo_preds(results, self.input_size).to(input_images.device)\n",
    "            predicted_masks.append(pmask)\n",
    "        predicted_masks = torch.stack(predicted_masks, dim=0)\n",
    "\n",
    "        # Combine input with predicted masks\n",
    "        combined_input = torch.cat([input_images, predicted_masks], dim=1)\n",
    "\n",
    "        # Encoder\n",
    "        x1 = self.inc(combined_input)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        # Latent space\n",
    "        x_flat = x5.view(batch_size, -1)\n",
    "        mu = torch.clamp(self.fc_mu(x_flat), -10, 10)\n",
    "        logvar = torch.clamp(self.fc_logvar(x_flat), -10, 10)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        # Decoder\n",
    "        x_decoded = self.fc_dec(z).view(batch_size, 1024, x5.size(2), x5.size(3))\n",
    "        x = self.up1(x_decoded, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        reconstruction = self.outc(x)\n",
    "\n",
    "        # Apply mask blending\n",
    "        predicted_masks_3 = predicted_masks.expand_as(reconstruction)\n",
    "        final_output = input_images * (1 - predicted_masks_3) + reconstruction * predicted_masks_3\n",
    "\n",
    "        return final_output, mu, logvar, predicted_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18bd5c-acce-44c0-a742-45ae89fee1a6",
   "metadata": {},
   "source": [
    "### Load Trained Model Function\n",
    "load_trained_model function initializes a pre-trained Variational Autoencoder (VAE) model with specified parameters. It sets the model to evaluation mode and loads the saved state dictionary from the given model_path. The model is designed to process images with specific configurations, including integration with a YOLO model for additional processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6295906-a8cf-4792-8e40-3d7d60d2193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path, device):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained VAE model with specified configurations.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path (str): Path to the saved model state dictionary.\n",
    "    - device (torch.device): The device (CPU or GPU) to load the model onto.\n",
    "\n",
    "    Returns:\n",
    "    - model (VAE): The VAE model loaded with pre-trained weights, ready for inference.\n",
    "    \"\"\"\n",
    "    # Initialize the VAE model with predefined parameters\n",
    "    model = VAE(\n",
    "        in_channels=4,         # Number of input channels (e.g., RGBA)\n",
    "        out_channels=3,        # Number of output channels (e.g., RGB)\n",
    "        latent_dim=256,        # Dimensionality of the latent space\n",
    "        input_size=128,        # Size of the input image (128x128)\n",
    "        yolo_model_path=\"yolo_best_model/best.pt\",  # Path to the YOLO model\n",
    "        device=device          # Device to use (CPU or GPU)\n",
    "    )\n",
    "    \n",
    "    # Load the model's state dictionary from the specified path\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    \n",
    "    # Set the model to evaluation mode to disable dropout and batch normalization\n",
    "    model.eval()\n",
    "    \n",
    "    # Return the loaded model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be7cd3-3558-4ae1-bb3d-ede03611ad8d",
   "metadata": {},
   "source": [
    "### Remove Watermark And Display`\n",
    "\n",
    "The `remove_watermark_and_display` function visualizes and saves the results of a watermark removal process using a trained model. It takes a watermarked image, an original clean image for reference, and processes the watermarked image through the model to produce a reconstructed image and a predicted watermark mask. The function displays these outputs side by side (watermarked image, predicted mask, reconstructed image, and original image) and saves the visualization in the `visualizator_outputs` directory with the model name prefixed to the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9280c197-88f7-49ff-beed-848d6d8379c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, model_name):\n",
    "    \"\"\"\n",
    "    Visualizes and saves the results of watermark removal, including the watermarked image,\n",
    "    predicted watermark mask, reconstructed image, and the original (clean) image.\n",
    "\n",
    "    Parameters:\n",
    "    - model (torch.nn.Module): The trained model for watermark removal.\n",
    "    - watermarked_image_path (str): Path to the watermarked image file.\n",
    "    - original_image_path (str): Path to the original clean image file.\n",
    "    - transform (callable): Transformation to apply to the input image.\n",
    "    - device (torch.device): Device (CPU or GPU) to use for inference.\n",
    "    - model_name (str): Name of the model, used to prefix the output file name.\n",
    "    \"\"\"\n",
    "    # Load and preprocess the watermarked and original images\n",
    "    watermarked_image = Image.open(watermarked_image_path).convert('RGB')  # Load watermarked image\n",
    "    original_image = Image.open(original_image_path).convert('RGB')        # Load original image\n",
    "    input_image = transform(watermarked_image).unsqueeze(0)               # Apply transformations and add batch dimension\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get model predictions for the input image\n",
    "        reconstructed_image, _, _, predicted_mask = model(input_image)\n",
    "    \n",
    "    # Post-process the model outputs for visualization\n",
    "    reconstructed_image = reconstructed_image.squeeze(0).cpu().clamp(0, 1)  # Convert output to [0, 1] range\n",
    "    predicted_mask = predicted_mask.squeeze(0).squeeze(0).cpu()             # Convert mask to CPU\n",
    "    input_image_for_display = input_image.squeeze(0).permute(1, 2, 0).cpu()  # Prepare input image for display\n",
    "    \n",
    "    # Create a side-by-side visualization with four subplots\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "    axs[0].imshow(watermarked_image)\n",
    "    axs[0].set_title('Watermarked Image')  # Watermarked input image\n",
    "    axs[1].imshow(predicted_mask, cmap='gray')\n",
    "    axs[1].set_title('Predicted Watermark Mask')  # Predicted watermark mask\n",
    "    axs[2].imshow(transforms.ToPILImage()(reconstructed_image))\n",
    "    axs[2].set_title('Watermark Removed Image')  # Reconstructed clean image\n",
    "    axs[3].imshow(original_image)\n",
    "    axs[3].set_title('Original Image')  # Ground truth clean image\n",
    "\n",
    "    # Hide axes for a cleaner display\n",
    "    for ax in axs:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Save the output visualization with the model name prefixed\n",
    "    output_dir = \"visualizator_outputs\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "    image_name = os.path.basename(watermarked_image_path)  # Extract the base name of the watermarked image\n",
    "    output_name = f\"{model_name}_{image_name}\"  # Prefix the image name with the model name\n",
    "    output_path = os.path.join(output_dir, output_name)\n",
    "    plt.savefig(output_path, bbox_inches='tight')  # Save the visualization to the specified path\n",
    "    plt.close(fig)  # Close the figure to release resources\n",
    "\n",
    "    print(f\"Visualization saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4a3c4-2248-490e-ae3b-03849b3122f5",
   "metadata": {},
   "source": [
    "### Show Model Outputs\n",
    "This code demonstrates loading pre-trained VAE models to remove watermarks from images and visualize the results. Two models are used: one trained on low-opacity watermarks and another on high-opacity watermarks. The script processes specific test images, displaying the watermarked image, predicted mask, reconstructed image, and original clean image side by side. The visualizations are saved with filenames indicating the model and image details for clear comparison. The device (CPU or GPU) is automatically selected based on availability, and input images are resized and transformed to match the model requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abd1f61a-6ea7-4447-b571-7d09d2b95db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved to visualizator_outputs\\no_logo_and_low_opacity_frame_3167_frame_3167_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\no_logo_and_low_opacity_frame_1325_frame_1325_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\no_logo_and_low_opacity_frame_2651_frame_2651_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\no_logo_and_low_opacity_frame_96_frame_96_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\no_logo_and_high_opacity_frame_3167_frame_3167_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\no_logo_and_high_opacity_frame_1325_frame_1325_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\no_logo_and_high_opacity_frame_2651_frame_2651_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\no_logo_and_high_opacity_frame_96_frame_96_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\logo_and_high_opacity_frame_3167_frame_3167_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\logo_and_high_opacity_frame_1325_frame_1325_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\logo_and_high_opacity_frame_2651_frame_2651_watermarked.jpg\n",
      "Visualization saved to visualizator_outputs\\logo_and_high_opacity_frame_96_frame_96_watermarked.jpg\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the image transformation pipeline\n",
    "# Images will be resized to 128x128 and converted to tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "model_path = 'outputs/VAE_NoLogoLowOpacity_128px_5folds_final.pt'\n",
    "model = load_trained_model(model_path, device)\n",
    "\n",
    "number = 3167\n",
    "watermarked_image_path = f'no_logo_and_low_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'no_logo_and_low_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"no_logo_and_low_opacity_frame_{number}\")\n",
    "\n",
    "number = 1325\n",
    "watermarked_image_path = f'no_logo_and_low_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'no_logo_and_low_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"no_logo_and_low_opacity_frame_{number}\")\n",
    "\n",
    "number = 2651\n",
    "watermarked_image_path = f'no_logo_and_low_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'no_logo_and_low_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"no_logo_and_low_opacity_frame_{number}\")\n",
    "\n",
    "number = 96\n",
    "watermarked_image_path = f'no_logo_and_low_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'no_logo_and_low_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"no_logo_and_low_opacity_frame_{number}\")\n",
    "\n",
    "model_path = 'outputs/VAE_NoLogoHighOpacity_128px_5folds_final.pt'\n",
    "model = load_trained_model(model_path, device)\n",
    "\n",
    "number = 3167\n",
    "watermarked_image_path = f'no_logo_and_high_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'no_logo_and_high_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"no_logo_and_high_opacity_frame_{number}\")\n",
    "\n",
    "number = 1325\n",
    "watermarked_image_path = f'no_logo_and_high_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'no_logo_and_high_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"no_logo_and_high_opacity_frame_{number}\")\n",
    "\n",
    "number = 2651\n",
    "watermarked_image_path = f'no_logo_and_high_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'no_logo_and_high_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"no_logo_and_high_opacity_frame_{number}\")\n",
    "\n",
    "number = 96\n",
    "watermarked_image_path = f'no_logo_and_high_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'no_logo_and_high_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"no_logo_and_high_opacity_frame_{number}\")\n",
    "\n",
    "model_path = 'outputs/VAE_LogoHighOpacity_128px_5folds_final.pt'\n",
    "model = load_trained_model(model_path, device)\n",
    "\n",
    "number = 3167\n",
    "watermarked_image_path = f'logo_and_high_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'no_logo_and_high_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"logo_and_high_opacity_frame_{number}\")\n",
    "\n",
    "number = 1325\n",
    "watermarked_image_path = f'logo_and_high_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'logo_and_high_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"logo_and_high_opacity_frame_{number}\")\n",
    "\n",
    "number = 2651\n",
    "watermarked_image_path = f'logo_and_high_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'logo_and_high_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"logo_and_high_opacity_frame_{number}\")\n",
    "\n",
    "number = 96\n",
    "watermarked_image_path = f'logo_and_high_opacity_watermark_dataset_test/frame_{number}_watermarked.jpg'\n",
    "original_image_path = f'logo_and_high_opacity_watermark_dataset_test/frame_{number}.jpg'\n",
    "remove_watermark_and_display(model, watermarked_image_path, original_image_path, transform, device, f\"logo_and_high_opacity_frame_{number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f1453a-e7cf-4721-a806-16ad264a72e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-example]",
   "language": "python",
   "name": "conda-env-.conda-example-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
